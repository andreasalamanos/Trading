{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import findspark\n",
    "findspark.init()\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.ml.feature import Tokenizer, RegexTokenizer\n",
    "from pyspark.ml.feature import CountVectorizer\n",
    "import pandas as pd\n",
    "from pyspark.ml.feature import HashingTF, IDF, Tokenizer, CountVectorizer\n",
    "from pyspark.ml.feature import StringIndexer\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "from pyspark.sql import Row\n",
    "from pyspark.ml.classification import NaiveBayes, NaiveBayesModel\n",
    "from pyspark.ml.feature import HashingTF, IDF, Tokenizer\n",
    "from pyspark.ml.feature import StringIndexer\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import StopWordsRemover\n",
    "import requests\n",
    "from pyspark.ml.pipeline import PipelineModel\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "from pyspark import SparkContext\n",
    "from nltk.tokenize import WordPunctTokenizer\n",
    "tok = WordPunctTokenizer()\n",
    "from pyspark.streaming import StreamingContext\n",
    "import traceback\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, NumericType, ArrayType, DoubleType\n",
    "import pandas as pd\n",
    "import sys\n",
    "from pyspark.sql import SQLContext, SparkSession\n",
    "from nltk.corpus import stopwords\n",
    "from operator import add\n",
    "from pyspark import SparkContext\n",
    "from pyspark import SparkConf\n",
    "\n",
    "\n",
    "sc = SparkContext(appName  = 'test')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "\n",
    "pattern = r'|'.join((r'@[A-Za-z0-9_]+', r'https?://[^ ]+'))\n",
    "web = r'www.[^ ]+'\n",
    "negations_dic = {\"isn't\":\"is not\", \"aren't\":\"are not\", \"wasn't\":\"was not\", \"weren't\":\"were not\",\n",
    "                \"haven't\":\"have not\",\"hasn't\":\"has not\",\"hadn't\":\"had not\",\"won't\":\"will not\",\n",
    "                \"wouldn't\":\"would not\", \"don't\":\"do not\", \"doesn't\":\"does not\",\"didn't\":\"did not\",\n",
    "                \"can't\":\"can not\",\"couldn't\":\"could not\",\"shouldn't\":\"should not\",\"mightn't\":\"might not\",\n",
    "                \"mustn't\":\"must not\"}\n",
    "neg_pattern = re.compile(r'\\b(' + '|'.join(negations_dic.keys()) + r')\\b')\n",
    "\n",
    "\n",
    "# Import Saved pre trained  model/Pipeline\n",
    "model =  PipelineModel.load('./trained_model/')\n",
    "\n",
    "IP = \"localhost\"\n",
    "Port = 9999\n",
    "\n",
    "sqlContext = SQLContext(sc)\n",
    "ssc = StreamingContext(sc, 20)\n",
    "\n",
    "\n",
    "\n",
    "stop_words = list(stopwords.words('english'))\n",
    "\n",
    "\n",
    "def clean( text):\n",
    "\n",
    "    soup = BeautifulSoup(text)\n",
    "    souped = soup.get_text()\n",
    "    try:\n",
    "        bom_removed = souped.decode(\"utf-8-sig\").replace(u\"\\ufffd\", \"?\")\n",
    "    except:\n",
    "        bom_removed = souped\n",
    "    stripped = re.sub(pattern, '', bom_removed)\n",
    "    stripped = re.sub(web, '', stripped)\n",
    "    lower_case = stripped.lower()\n",
    "    neg_handled = neg_pattern.sub(lambda x: negations_dic[x.group()], lower_case)\n",
    "    alphar = re.sub(\"[^a-zA-Z]\", \" \", neg_handled)\n",
    "    words = \" \".join([x.lower() for x in tok.tokenize(alphar) if len(x) > 1 ])\n",
    "    words = [x for x in words.split(' ') if not x in stop_words ]\n",
    "    return (\" \".join(words)).strip()\n",
    "\n",
    "\n",
    "\n",
    "def infere(tweet_text):\n",
    "    try:\n",
    "        if not tweet_text.isEmpty():\n",
    "            data_df = sqlContext.createDataFrame(tweet_text.collect()[0], StringType(), False)\n",
    "            data_df = data_df.selectExpr(\"value as text\")\n",
    "            data_df.show()\n",
    "            prediction = model.transform(data_df)\n",
    "            prediction = prediction.select('text','prediction')\n",
    "            prediction.show()\n",
    "            return prediction.filter(prediction.text.isNotNull())\n",
    "        \n",
    "    except:\n",
    "        import traceback\n",
    "        e= traceback()\n",
    "        print('exception', e)\n",
    "    \n",
    "def bigram(words):\n",
    "    bigrams = []\n",
    "    for i in range(len(words) - 1):\n",
    "        bigrams.append((tuple(words[i:i+2]), 1))\n",
    "    return bigrams\n",
    "\n",
    "\n",
    "\n",
    "lines = ssc.socketTextStream(IP, Port) \n",
    "line = lines.map(lambda cleaned: clean(cleaned))\n",
    "line = line.map(lambda line: line.split('mydelimiter'))\n",
    "line2 = line.foreachRDD( lambda rdd_: infere( rdd_).write.csv(\"./out/\", mode = 'append') if not rdd_.isEmpty() else None )\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Start the computation\n",
    "ssc.start()             \n",
    "# Wait for the computation to terminate\n",
    "ssc.awaitTerminationOrTimeout(300)  \n",
    "\n",
    "ssc.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "import pandas as pd\n",
    "\n",
    "spark = SparkSession.builder.appName('test').getOrCreate()\n",
    "df=spark.read.csv(\n",
    "    \"./out/*.csv\",\\\n",
    "    header=False, mode = 'append')\n",
    "\n",
    "df.printSchema()\n",
    "df = df.selectExpr(\"_c0 as text\", \"_c1 as Score\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot the top words "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "negative_oc = df.filter(df.Score==0).withColumn('NegativeOccurencies', explode(split(col('text'), ' ')))\\\n",
    "    .groupBy('NegativeOccurencies')\\\n",
    "    .count()\\\n",
    "    .sort('count', ascending=False)\n",
    "\n",
    "positive_oc = df.filter(df.Score==1).withColumn('PositiveOccurencies', explode(split(col('text'), ' ')))\\\n",
    "    .groupBy('PositiveOccurencies')\\\n",
    "    .count()\\\n",
    "    .sort('count', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Number of tweets classified as positive: {} \\nNumber of tewwts classified as Negative: {}'.format(positive_oc.count(), negative_oc.count()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "fig, ax = plt.subplots(figsize = (12,10), nrows=1, ncols=2)\n",
    "positive_oc.toPandas().head(10).plot.bar(x='PositiveOccurencies', y='count', ax = ax[0])\n",
    "negative_oc.toPandas().head(10).plot.bar(x='NegativeOccurencies', y='count', ax=ax[1] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
